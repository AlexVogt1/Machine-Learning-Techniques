{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# miniLLM",
   "id": "83c618941bccaa5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "4cec801c04df123c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T08:20:19.473447Z",
     "start_time": "2025-10-27T08:20:19.465683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import pickle"
   ],
   "id": "90b3bd73cc126245",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "198bd7754bb502aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T08:20:19.498754Z",
     "start_time": "2025-10-27T08:20:19.489914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Simple character-level tokenizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def fit(self, text: str):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(chars)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return [self.char_to_idx[ch] for ch in text if ch in self.char_to_idx]\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        return ''.join([self.idx_to_char[i] for i in indices])"
   ],
   "id": "a09a9b911d80a558",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Milti-head Attention\n",
    "Allows the model to focus on different parts of the imput"
   ],
   "id": "3aa1979754e83c83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T08:20:19.513620Z",
     "start_time": "2025-10-27T08:20:19.498754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.01\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.01\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "\n",
    "        attention_weights = self.softmax(scores)\n",
    "        output = np.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Linear projections\n",
    "        Q = np.matmul(x, self.W_q)\n",
    "        K = np.matmul(x, self.W_k)\n",
    "        V = np.matmul(x, self.W_v)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_output, _ = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = np.matmul(attn_output, self.W_o)\n",
    "        return output"
   ],
   "id": "f982c4e10ceefd45",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feed-Forward Network\n",
    "Position wise transform applied after attention"
   ],
   "id": "39a73b5db6157810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T08:20:19.523376Z",
     "start_time": "2025-10-27T08:20:19.517812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward:\n",
    "    \"\"\"Position-wise feed-forward network\"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) * 0.01\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) * 0.01\n",
    "        self.b2 = np.zeros(d_model)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.relu(np.matmul(x, self.W1) + self.b1)\n",
    "        output = np.matmul(hidden, self.W2) + self.b2\n",
    "        return output"
   ],
   "id": "a449a10c483b38fb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer Block",
   "id": "af173a1cf5a1cd9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T08:20:19.535940Z",
     "start_time": "2025-10-27T08:20:19.523376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock:\n",
    "    \"\"\"Single transformer decoder block\"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Layer norm parameters\n",
    "        self.gamma1 = np.ones(d_model)\n",
    "        self.beta1 = np.zeros(d_model)\n",
    "        self.gamma2 = np.ones(d_model)\n",
    "        self.beta2 = np.zeros(d_model)\n",
    "\n",
    "    def layer_norm(self, x, gamma, beta, eps=1e-6):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        return gamma * (x - mean) / np.sqrt(var + eps) + beta\n",
    "\n",
    "    def forward(self, x, mask=None, training=False):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.attention.forward(x, mask)\n",
    "        x = self.layer_norm(x + attn_output, self.gamma1, self.beta1)\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn.forward(x)\n",
    "        x = self.layer_norm(x + ffn_output, self.gamma2, self.beta2)\n",
    "\n",
    "        return x"
   ],
   "id": "cfebca47c28c0b42",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MiniLLM",
   "id": "a54c82ee0083aea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T08:20:19.557879Z",
     "start_time": "2025-10-27T08:20:19.535940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerLM:\n",
    "    \"\"\"Complete transformer language model\"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, num_heads: int = 4,\n",
    "                 num_layers: int = 4, d_ff: int = 512, max_seq_len: int = 256):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Token embeddings\n",
    "        self.token_embedding = np.random.randn(vocab_size, d_model) * 0.01\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoding = self._create_positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = [TransformerBlock(d_model, num_heads, d_ff)\n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = np.random.randn(d_model, vocab_size) * 0.01\n",
    "\n",
    "        # Store all parameters for optimization\n",
    "        self.params = []\n",
    "        self._collect_parameters()\n",
    "\n",
    "    def _create_positional_encoding(self, max_len: int, d_model: int):\n",
    "        pos_enc = np.zeros((max_len, d_model))\n",
    "        position = np.arange(0, max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "        return pos_enc\n",
    "\n",
    "    def _collect_parameters(self):\n",
    "        \"\"\"Collect all trainable parameters\"\"\"\n",
    "        self.params = [self.token_embedding, self.output_proj]\n",
    "        for block in self.blocks:\n",
    "            self.params.extend([\n",
    "                block.attention.W_q, block.attention.W_k,\n",
    "                block.attention.W_v, block.attention.W_o,\n",
    "                block.ffn.W1, block.ffn.b1, block.ffn.W2, block.ffn.b2,\n",
    "                block.gamma1, block.beta1, block.gamma2, block.beta2\n",
    "            ])\n",
    "\n",
    "    def _create_causal_mask(self, seq_len: int):\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "        mask = mask * -1e9\n",
    "        return mask\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # Embed tokens\n",
    "        x_embed = self.token_embedding[x]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x_embed = x_embed + self.pos_encoding[:seq_len]\n",
    "\n",
    "        # Create causal mask\n",
    "        mask = self._create_causal_mask(seq_len)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x_embed = block.forward(x_embed, mask, training)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        logits = np.matmul(x_embed, self.output_proj)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, start_tokens: List[int], max_new_tokens: int = 100,\n",
    "                 temperature: float = 1.0) -> List[int]:\n",
    "        tokens = start_tokens.copy()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get context (last max_seq_len tokens)\n",
    "            context = tokens[-self.max_seq_len:]\n",
    "            x = np.array([context])\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self.forward(x, training=False)\n",
    "\n",
    "            # Get logits for last token\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "\n",
    "            # Apply softmax\n",
    "            probs = self.softmax(next_token_logits)\n",
    "\n",
    "            # Sample from distribution\n",
    "            next_token = np.random.choice(self.vocab_size, p=probs)\n",
    "            tokens.append(next_token)\n",
    "\n",
    "        return tokens"
   ],
   "id": "1ae2a7da0d8faa72",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trainer Class",
   "id": "7f87b4295382bc4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T08:20:19.581238Z",
     "start_time": "2025-10-27T08:20:19.557879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    \"\"\"Training loop with backpropagation\"\"\"\n",
    "    def __init__(self, model: TransformerLM, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Adam optimizer parameters\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.eps = 1e-8\n",
    "\n",
    "        # Initialize momentum and velocity for each parameter\n",
    "        self.m = [np.zeros_like(p) for p in model.params]\n",
    "        self.v = [np.zeros_like(p) for p in model.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def compute_loss(self, logits, targets):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "        # Flatten logits and targets\n",
    "        logits_flat = logits.reshape(-1, vocab_size)\n",
    "        targets_flat = targets.reshape(-1)\n",
    "\n",
    "        # Compute softmax\n",
    "        probs = self.softmax(logits_flat)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        loss = -np.mean(np.log(probs[np.arange(len(targets_flat)), targets_flat] + 1e-10))\n",
    "        return loss, probs\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def compute_gradients_numerical(self, x, y, epsilon=1e-4):\n",
    "        \"\"\"Numerical gradient computation (simplified for demonstration)\"\"\"\n",
    "        gradients = []\n",
    "        original_loss, _ = self.compute_loss(self.model.forward(x), y)\n",
    "\n",
    "        for i, param in enumerate(self.model.params):\n",
    "            grad = np.zeros_like(param)\n",
    "\n",
    "            # Sample a few random indices for efficiency\n",
    "            indices = np.random.choice(param.size, min(100, param.size), replace=False)\n",
    "\n",
    "            for idx in indices:\n",
    "                idx_tuple = np.unravel_index(idx, param.shape)\n",
    "\n",
    "                # Finite difference\n",
    "                param[idx_tuple] += epsilon\n",
    "                loss_plus, _ = self.compute_loss(self.model.forward(x), y)\n",
    "                param[idx_tuple] -= epsilon\n",
    "\n",
    "                grad[idx_tuple] = (loss_plus - original_loss) / epsilon\n",
    "\n",
    "            gradients.append(grad)\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def update_parameters(self, gradients):\n",
    "        \"\"\"Adam optimizer update\"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(self.model.params, gradients)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameters\n",
    "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    def prepare_batches(self, encoded_text: List[int], batch_size: int, seq_len: int):\n",
    "        \"\"\"Prepare training batches\"\"\"\n",
    "        batches = []\n",
    "\n",
    "        for i in range(0, len(encoded_text) - seq_len - 1, seq_len):\n",
    "            if len(batches) >= batch_size:\n",
    "                break\n",
    "\n",
    "            x = encoded_text[i:i + seq_len]\n",
    "            y = encoded_text[i + 1:i + seq_len + 1]\n",
    "\n",
    "            if len(x) == seq_len and len(y) == seq_len:\n",
    "                batches.append((np.array(x), np.array(y)))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def train(self, text: str, tokenizer: Tokenizer, epochs: int = 10,\n",
    "              batch_size: int = 4, seq_len: int = 32):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        encoded = tokenizer.encode(text)\n",
    "        print(f\"Training on {len(encoded)} tokens for {epochs} epochs\\n\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Prepare batches\n",
    "            batches = self.prepare_batches(encoded, batch_size, seq_len)\n",
    "\n",
    "            if not batches:\n",
    "                print(\"Not enough data for batches!\")\n",
    "                break\n",
    "\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for batch_idx, (x, y) in enumerate(batches):\n",
    "                # Add batch dimension\n",
    "                x_batch = x.reshape(1, -1)\n",
    "                y_batch = y.reshape(1, -1)\n",
    "\n",
    "                # Forward pass\n",
    "                logits = self.model.forward(x_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss, _ = self.compute_loss(logits, y_batch)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Backward pass (simplified numerical gradients)\n",
    "                if batch_idx == 0:  # Only update on first batch for efficiency\n",
    "                    gradients = self.compute_gradients_numerical(x_batch, y_batch)\n",
    "                    self.update_parameters(gradients)\n",
    "\n",
    "            avg_loss = epoch_loss / len(batches)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Generate sample text every few epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.generate_sample(tokenizer)\n",
    "\n",
    "    def generate_sample(self, tokenizer: Tokenizer):\n",
    "        \"\"\"Generate and print a sample\"\"\"\n",
    "        start_text = \"Hello\"\n",
    "        start_tokens = tokenizer.encode(start_text)\n",
    "        generated = self.model.generate(start_tokens, max_new_tokens=50, temperature=0.8)\n",
    "        text = tokenizer.decode(generated)\n",
    "        print(f\"\\nGenerated sample:\\n{text}\\n\")"
   ],
   "id": "56515c6c3b7d4636",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Demonstraition",
   "id": "124db7ba90112bad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T08:25:51.335698Z",
     "start_time": "2025-10-27T08:24:40.899881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample training text (use more data for better results)\n",
    "text = \"\"\"Hello world! This is a simple transformer language model built from scratch.\n",
    "It can learn to generate text by predicting the next character.\n",
    "The model uses multi-head attention and feed-forward networks.\n",
    "With enough training data, it can learn patterns in language.\n",
    "Hello world! Machine learning is fascinating and powerful.\n",
    "Neural networks can learn complex patterns from data.\n",
    "The transformer architecture revolutionized natural language processing.\n",
    "Attention mechanisms allow models to focus on relevant information.\n",
    "Deep learning has achieved remarkable results in many domains.\n",
    "\"\"\" * 5  # Repeat for more training data\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(text)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Training text length: {len(text)} characters\")\n",
    "\n",
    "# Initialize model (smaller for faster training)\n",
    "model = TransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=32,\n",
    "    num_heads=2,\n",
    "    num_layers=2,\n",
    "    d_ff=64,\n",
    "    max_seq_len=64\n",
    ")\n",
    "\n",
    "print(f\"\\nModel initialized\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(model, learning_rate=0.01)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 60)\n",
    "trainer.train(\n",
    "    text=text,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    seq_len=32\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate final samples\n",
    "print(\"\\nFinal generation samples:\\n\")\n",
    "for temp in [0.5, 0.8, 1.0]:\n",
    "    start_text = \"Hello\"\n",
    "    start_tokens = tokenizer.encode(start_text)\n",
    "    generated = model.generate(start_tokens, max_new_tokens=100, temperature=temp)\n",
    "    text_out = tokenizer.decode(generated)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"{text_out}\\n\")"
   ],
   "id": "4da5fcaf042ea6ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 38\n",
      "Training text length: 2915 characters\n",
      "\n",
      "Model initialized\n",
      "============================================================\n",
      "\n",
      "Starting training...\n",
      "============================================================\n",
      "Training on 2915 tokens for 50 epochs\n",
      "\n",
      "Epoch 1/50 - Loss: 3.6239\n",
      "Epoch 2/50 - Loss: 3.6069\n",
      "Epoch 3/50 - Loss: 3.5896\n",
      "Epoch 4/50 - Loss: 3.5710\n",
      "Epoch 5/50 - Loss: 3.5506\n",
      "\n",
      "Generated sample:\n",
      "HelloH!rgdkv,ppadco\n",
      "!HdT!tH,sf!o-zgzH!bDtTD!.M-kri arsW\n",
      "\n",
      "Epoch 6/50 - Loss: 3.5274\n",
      "Epoch 7/50 - Loss: 3.5037\n",
      "Epoch 8/50 - Loss: 3.4788\n",
      "Epoch 9/50 - Loss: 3.4536\n",
      "Epoch 10/50 - Loss: 3.4292\n",
      "\n",
      "Generated sample:\n",
      "HelloirWtdtiiMceueDaoediv looa\n",
      "rTuN!HTrriw Nicibn lTdvz\n",
      "\n",
      "Epoch 11/50 - Loss: 3.4065\n",
      "Epoch 12/50 - Loss: 3.3860\n",
      "Epoch 13/50 - Loss: 3.3696\n",
      "Epoch 14/50 - Loss: 3.3576\n",
      "Epoch 15/50 - Loss: 3.3479\n",
      "\n",
      "Generated sample:\n",
      "HellowwDtTadThiwI.osl m  tl.tlits  Tw MroInsrItdIrvamva\n",
      "\n",
      "Epoch 16/50 - Loss: 3.3432\n",
      "Epoch 17/50 - Loss: 3.3413\n",
      "Epoch 18/50 - Loss: 3.3438\n",
      "Epoch 19/50 - Loss: 3.3511\n",
      "Epoch 20/50 - Loss: 3.3624\n",
      "\n",
      "Generated sample:\n",
      "Helloaoelsei! sheTtr !eiW sk,rtNgie ta r iarrs se s!io \n",
      "\n",
      "Epoch 21/50 - Loss: 3.3769\n",
      "Epoch 22/50 - Loss: 3.3916\n",
      "Epoch 23/50 - Loss: 3.4060\n",
      "Epoch 24/50 - Loss: 3.4225\n",
      "Epoch 25/50 - Loss: 3.4407\n",
      "\n",
      "Generated sample:\n",
      "HelloirelddsTezvms i   sspamaoaemtu.s i\n",
      "al v\n",
      "ma!iraetaa\n",
      "\n",
      "Epoch 26/50 - Loss: 3.4616\n",
      "Epoch 27/50 - Loss: 3.4856\n",
      "Epoch 28/50 - Loss: 3.5134\n",
      "Epoch 29/50 - Loss: 3.5427\n",
      "Epoch 30/50 - Loss: 3.5706\n",
      "\n",
      "Generated sample:\n",
      "Hellololcroiima ,iimsa sapisotirWcdlliHli s mpae !rpl d\n",
      "\n",
      "Epoch 31/50 - Loss: 3.5931\n",
      "Epoch 32/50 - Loss: 3.6150\n",
      "Epoch 33/50 - Loss: 3.6407\n",
      "Epoch 34/50 - Loss: 3.6735\n",
      "Epoch 35/50 - Loss: 3.7200\n",
      "\n",
      "Generated sample:\n",
      "Helloiddel!rplTs a   assimrl irtrar\n",
      "e w aTlm meleotp i \n",
      "\n",
      "Epoch 36/50 - Loss: 3.7705\n",
      "Epoch 37/50 - Loss: 3.8187\n",
      "Epoch 38/50 - Loss: 3.8605\n",
      "Epoch 39/50 - Loss: 3.8965\n",
      "Epoch 40/50 - Loss: 3.9380\n",
      "\n",
      "Generated sample:\n",
      "Hellowrood!!!Thii    reimmxel tTbmaaasaraelltr\n",
      "k.tppae \n",
      "\n",
      "Epoch 41/50 - Loss: 3.9939\n",
      "Epoch 42/50 - Loss: 4.0489\n",
      "Epoch 43/50 - Loss: 4.1002\n",
      "Epoch 44/50 - Loss: 4.1554\n",
      "Epoch 45/50 - Loss: 4.2044\n",
      "\n",
      "Generated sample:\n",
      "Hello h wld Thiiw sia a mpe p  tara tmIrae atp,elwrpsie\n",
      "\n",
      "Epoch 46/50 - Loss: 4.2444\n",
      "Epoch 47/50 - Loss: 4.3026\n",
      "Epoch 48/50 - Loss: 4.3861\n",
      "Epoch 49/50 - Loss: 4.4327\n",
      "Epoch 50/50 - Loss: 4.5358\n",
      "\n",
      "Generated sample:\n",
      "Hello wwold! Tis ssa ls sre p traa aiyaoatrxaoaplNpraar\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "============================================================\n",
      "\n",
      "Final generation samples:\n",
      "\n",
      "Temperature 0.5:\n",
      "Hello wo ld! This s a sis ple trara traa traaarelplra tttrebarpplr zcrNpottnpprrpppprprplrprprlDprbtrplbp\n",
      "\n",
      "Temperature 0.8:\n",
      "Hello world! o kiis a a mplle traaaia talHe taltzprpaa!mapla.pptNrn!rprpplpl\n",
      "HrelIppa\n",
      "o HpTpralx-wtryp pa\n",
      "\n",
      "Temperature 1.0:\n",
      "Hello wordd!s This s aA thao  r a ra -Hab pt otelt.pr ha azrgra,Dcktppltl\n",
      "pn,Nbzt!zpDmkppotplrgNtrpcprobf\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Thoughts\n",
    "This LLM doesn't perform very well but that is it be expected from a model with such a small training sample.\n",
    "I did learn alot form this, especially about transformers."
   ],
   "id": "ee3ae08cce2d8873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T08:20:45.685959Z",
     "start_time": "2025-10-27T08:20:45.682256Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8cf17ba5d7c2a797"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
