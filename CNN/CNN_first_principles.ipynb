{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convolutional Neural Networks from First Principles",
   "id": "3c7ae30542ecb9aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "3dd66420537c3516"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.539236Z",
     "start_time": "2025-10-21T11:15:39.534811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Minimal packages and libraries used\n",
    "import numpy as np"
   ],
   "id": "47b815555c4fe9df",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Convolutional Layer Class\n",
    "This class is used to create and add convolutional layers to our network"
   ],
   "id": "68d3797ca45b8262"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.582416Z",
     "start_time": "2025-10-21T11:15:39.566352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convolutional Layer\n",
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1\n",
    "        self.bias = np.zeros((out_channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        batch_size, _, h, w = x.shape\n",
    "\n",
    "        # Apply padding\n",
    "        if self.padding > 0:\n",
    "            x = np.pad(x, ((0,0), (0,0), (self.padding, self.padding),\n",
    "                          (self.padding, self.padding)), mode='constant')\n",
    "\n",
    "        # Calculate output dimensions\n",
    "        out_h = (h + 2*self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_w = (w + 2*self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Initialize output\n",
    "        output = np.zeros((batch_size, self.out_channels, out_h, out_w))\n",
    "\n",
    "        # Perform convolution\n",
    "        for b in range(batch_size):\n",
    "            for c_out in range(self.out_channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        receptive_field = x[b, :, h_start:h_end, w_start:w_end]\n",
    "                        output[b, c_out, i, j] = np.sum(receptive_field * self.weights[c_out]) + self.bias[c_out]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad, learning_rate=0.01):\n",
    "        batch_size, _, grad_h, grad_w = grad.shape\n",
    "\n",
    "        # Pad input for gradient calculation\n",
    "        x_padded = self.x\n",
    "        if self.padding > 0:\n",
    "            x_padded = np.pad(self.x, ((0,0), (0,0), (self.padding, self.padding),\n",
    "                                       (self.padding, self.padding)), mode='constant')\n",
    "\n",
    "        # Initialize gradients\n",
    "        d_weights = np.zeros_like(self.weights)\n",
    "        d_bias = np.zeros_like(self.bias)\n",
    "        d_x = np.zeros_like(x_padded)\n",
    "\n",
    "        # Calculate gradients\n",
    "        for b in range(batch_size):\n",
    "            for c_out in range(self.out_channels):\n",
    "                for i in range(grad_h):\n",
    "                    for j in range(grad_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        d_weights[c_out] += x_padded[b, :, h_start:h_end, w_start:w_end] * grad[b, c_out, i, j]\n",
    "                        d_bias[c_out] += grad[b, c_out, i, j]\n",
    "                        d_x[b, :, h_start:h_end, w_start:w_end] += self.weights[c_out] * grad[b, c_out, i, j]\n",
    "\n",
    "        # Update weights\n",
    "        self.weights -= learning_rate * d_weights / batch_size\n",
    "        self.bias -= learning_rate * d_bias / batch_size\n",
    "\n",
    "        # Remove padding from gradient if needed\n",
    "        if self.padding > 0:\n",
    "            d_x = d_x[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "        return d_x"
   ],
   "id": "8da6715ed21391c6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MaxPooling Layer Class\n",
    "This class is used to create and add MaxPooling Layers to our networks"
   ],
   "id": "514dbe7441c827ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.602759Z",
     "start_time": "2025-10-21T11:15:39.591450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Max Pooling Layer\n",
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        batch_size, channels, h, w = x.shape\n",
    "\n",
    "        out_h = (h - self.pool_size) // self.stride + 1\n",
    "        out_w = (w - self.pool_size) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((batch_size, channels, out_h, out_w))\n",
    "        self.max_indices = np.zeros_like(output, dtype=int)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.pool_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.pool_size\n",
    "\n",
    "                        pool_region = x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        output[b, c, i, j] = np.max(pool_region)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        d_x = np.zeros_like(self.x)\n",
    "        batch_size, channels, grad_h, grad_w = grad.shape\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(grad_h):\n",
    "                    for j in range(grad_w):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.pool_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.pool_size\n",
    "\n",
    "                        pool_region = self.x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        max_val = np.max(pool_region)\n",
    "                        mask = (pool_region == max_val)\n",
    "                        d_x[b, c, h_start:h_end, w_start:w_end] += grad[b, c, i, j] * mask\n",
    "\n",
    "        return d_x"
   ],
   "id": "fb7766010ad7945c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fully Connected (Dense) Layer Class\n",
    "This class is used to create and add dense Layers to our networks"
   ],
   "id": "be24727410043d12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.619674Z",
     "start_time": "2025-10-21T11:15:39.611806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fully Connected Layer\n",
    "class Dense:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, grad, learning_rate=0.01):\n",
    "        d_weights = np.dot(self.x.T, grad)\n",
    "        d_bias = np.sum(grad, axis=0, keepdims=True)\n",
    "        d_x = np.dot(grad, self.weights.T)\n",
    "\n",
    "        self.weights -= learning_rate * d_weights / self.x.shape[0]\n",
    "        self.bias -= learning_rate * d_bias / self.x.shape[0]\n",
    "\n",
    "        return d_x\n"
   ],
   "id": "43ba8423d96c6cf",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Activation Classes\n",
    "Create softmax, and relu activation functions"
   ],
   "id": "67a8a58662e7a450"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.632316Z",
     "start_time": "2025-10-21T11:15:39.625820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Activation Functions\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * (self.x > 0)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad"
   ],
   "id": "7fa3ea3f5ae57438",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.645854Z",
     "start_time": "2025-10-21T11:15:39.642375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flatten Layer\n",
    "class Flatten:\n",
    "    def forward(self, x):\n",
    "        self.shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad.reshape(self.shape)"
   ],
   "id": "16e07213ffded566",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.657862Z",
     "start_time": "2025-10-21T11:15:39.652864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss Function\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        batch_size = predictions.shape[0]\n",
    "\n",
    "        # Clip predictions to prevent log(0)\n",
    "        predictions = np.clip(predictions, 1e-10, 1 - 1e-10)\n",
    "        loss = -np.sum(targets * np.log(predictions)) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        return (self.predictions - self.targets)"
   ],
   "id": "eefdd349e00cf26",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## CNN Class\n",
    "Thhis class allows us to build, train a CNN and then allow us to predict from the trained CNN"
   ],
   "id": "c644148da16d7982"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.675455Z",
     "start_time": "2025-10-21T11:15:39.666945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2D(1, 8, 3, stride=1, padding=1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2D(2, 2)\n",
    "\n",
    "        self.conv2 = Conv2D(8, 16, 3, stride=1, padding=1)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2D(2, 2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(16 * 7 * 7, 128)  # Assuming 28x28 input\n",
    "        self.relu3 = ReLU()\n",
    "        self.fc2 = Dense(128, 10)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.pool1.forward(x)\n",
    "\n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.relu2.forward(x)\n",
    "        x = self.pool2.forward(x)\n",
    "\n",
    "        x = self.flatten.forward(x)\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu3.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backward(self, learning_rate=0.01):\n",
    "        grad = self.loss_fn.backward()\n",
    "\n",
    "        grad = self.softmax.backward(grad)\n",
    "        grad = self.fc2.backward(grad, learning_rate)\n",
    "        grad = self.relu3.backward(grad)\n",
    "        grad = self.fc1.backward(grad, learning_rate)\n",
    "        grad = self.flatten.backward(grad)\n",
    "\n",
    "        grad = self.pool2.backward(grad)\n",
    "        grad = self.relu2.backward(grad)\n",
    "        grad = self.conv2.backward(grad, learning_rate)\n",
    "\n",
    "        grad = self.pool1.backward(grad)\n",
    "        grad = self.relu1.backward(grad)\n",
    "        grad = self.conv1.backward(grad, learning_rate)\n",
    "\n",
    "    def train(self, x, y, learning_rate=0.01):\n",
    "        # Forward pass\n",
    "        predictions = self.forward(x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn.forward(predictions, y)\n",
    "\n",
    "        # Backward pass\n",
    "        self.backward(learning_rate)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        predictions = self.forward(x)\n",
    "        return np.argmax(predictions, axis=1)"
   ],
   "id": "bfae7bd891687d8e",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions",
   "id": "a93a6cb3c44d95aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:15:39.695465Z",
     "start_time": "2025-10-21T11:15:39.689463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper function to create synthetic patterns\n",
    "def create_synthetic_data(n_samples=500):\n",
    "    \"\"\"Create simple patterns: vertical lines (0) vs horizontal lines (1)\"\"\"\n",
    "    X = np.zeros((n_samples, 1, 28, 28))\n",
    "    y = np.zeros((n_samples, 10))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        label = i % 2  # Alternate between 0 and 1\n",
    "\n",
    "        if label == 0:  # Vertical line\n",
    "            col = np.random.randint(10, 18)\n",
    "            X[i, 0, 5:23, col:col+2] = 1.0\n",
    "        else:  # Horizontal line\n",
    "            row = np.random.randint(10, 18)\n",
    "            X[i, 0, row:row+2, 5:23] = 1.0\n",
    "\n",
    "        # Add small noise\n",
    "        X[i] += np.random.randn(1, 28, 28) * 0.1\n",
    "        y[i, label] = 1\n",
    "\n",
    "    return X, y"
   ],
   "id": "63cc4fbe9a9b0281",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:27:29.772344Z",
     "start_time": "2025-10-21T11:15:39.708011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example Usage\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic data with actual patterns\n",
    "print(\"Creating synthetic dataset (vertical vs horizontal lines)...\")\n",
    "X_train, y_train = create_synthetic_data(n_samples=500)\n",
    "X_test, y_test = create_synthetic_data(n_samples=100)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN()\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "batch_size = 10\n",
    "\n",
    "print(\"\\nTraining CNN from scratch...\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_x = X_train_shuffled[i:i+batch_size]\n",
    "        batch_y = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "        loss = model.train(batch_x, batch_y, learning_rate=0.01)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = model.predict(batch_x)\n",
    "        correct += np.sum(preds == np.argmax(batch_y, axis=1))\n",
    "        total += len(batch_x)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = epoch_loss / (len(X_train) // batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Test on unseen data\n",
    "print(\"\\n--- Testing on new data ---\")\n",
    "test_predictions = model.predict(X_test)\n",
    "test_labels = np.argmax(y_test, axis=1)\n",
    "test_accuracy = 100 * np.sum(test_predictions == test_labels) / len(test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Show some predictions\n",
    "print(f\"\\nFirst 10 predictions: {test_predictions[:10]}\")\n",
    "print(f\"Actual labels:        {test_labels[:10]}\")"
   ],
   "id": "dac70f0a46cf36fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic dataset (vertical vs horizontal lines)...\n",
      "\n",
      "Training CNN from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexv\\AppData\\Local\\Temp\\ipykernel_5352\\3650984177.py:41: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  output[b, c_out, i, j] = np.sum(receptive_field * self.weights[c_out]) + self.bias[c_out]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.5068, Accuracy: 62.20%\n",
      "Epoch 2/5, Loss: 0.5727, Accuracy: 83.80%\n",
      "Epoch 3/5, Loss: 0.2851, Accuracy: 98.20%\n",
      "Epoch 4/5, Loss: 0.0928, Accuracy: 100.00%\n",
      "Epoch 5/5, Loss: 0.0374, Accuracy: 100.00%\n",
      "\n",
      "--- Testing on new data ---\n",
      "Test Accuracy: 100.00%\n",
      "\n",
      "First 10 predictions: [0 1 0 1 0 1 0 1 0 1]\n",
      "Actual labels:        [0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
